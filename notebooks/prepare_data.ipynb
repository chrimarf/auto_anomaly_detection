{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data to compare our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_covtype, fetch_kddcup99\n",
    "from sklearn.datasets.mldata import fetch_mldata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and editing datasets\n",
    "\n",
    "The target variable contains the label of abnormality.\n",
    "\n",
    "0 : Normal\n",
    "\n",
    "1 : Anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covtype = fetch_covtype()\n",
    "SF = fetch_kddcup99(subset = 'SF')\n",
    "http = fetch_kddcup99(subset = 'http')\n",
    "shuttle = fetch_mldata('shuttle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the rules proposed in [Learning hyperparameters for unsupervised anomaly detection. A. Thomas, S. Clémençon, V. Feuillard, A. Gramfort. Anomaly Detection Workshop, ICML 2016](https://drive.google.com/file/d/0B8Dg3PBX90KNUTg5NGNOVnFPX0hDNmJsSTcybzZMSHNPYkd3/view).\n",
    "\n",
    "For the Forest Cover dataset cover types 4 and 5 are considered abnormal when the cover type 2 is considered as normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covtype = pd.DataFrame(covtype.data)\n",
    "df_covtype['target'] = covtype.target\n",
    "df_covtype = df_covtype.query('target in [2,4,5]')\n",
    "df_covtype.target = df_covtype.target.replace(2,0).replace(4,1).replace(5,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sf and http dataset all the categories not flagged normal are considered abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf = pd.DataFrame(SF.data)\n",
    "df_sf['target'] = SF.target\n",
    "df_sf.target[df_sf.target != 'normal.'] = 1\n",
    "df_sf.target = df_sf.target.replace('normal.',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_http = pd.DataFrame(http.data)\n",
    "df_http['target'] = http.target\n",
    "df_http.target[df_http.target != 'normal.'] = 1\n",
    "df_http.target = df_http.target.replace('normal.',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuttle = pd.DataFrame(shuttle.data)\n",
    "df_shuttle['target'] = shuttle.target\n",
    "df_shuttle = df_shuttle.query('target != 4')\n",
    "df_shuttle.target = df_shuttle.target.replace(1,0)\n",
    "df_shuttle.loc[df_shuttle.target != 0,'target'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud) deals with fraudulent activities on credit cards and has been released with [Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015 ](https://www3.nd.edu/~rjohns15/content/papers/ssci2015_calibrating.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_creditcard = pd.read_csv('../data/creditcard.csv')\n",
    "df_creditcard = df_creditcard.rename(columns={\"Class\":\"target\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldfs = [var for var in dir() if (isinstance(eval(var), pd.core.frame.DataFrame)) and (var != 'stats_df') and (var[:2]=='df')]\n",
    "stats_df = pd.DataFrame(columns = ['name','ncol','nrow','anomaly_percentage'])\n",
    "stats_df['name'] = alldfs\n",
    "stats_df['ncol'] = [len(locals()[df].columns) for df in alldfs]\n",
    "stats_df['nrow'] = [len(locals()[df].index) for df in alldfs]\n",
    "stats_df['anomaly_percentage'] = [len((locals()[df]).query('target == 1').index) for df in alldfs]/stats_df['nrow']\n",
    "\n",
    "print 'Attributes of the datasets used'\n",
    "print '--------------------------------'\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_list = [locals()[df].to_pickle('../data/pickle_datasets/{name}.pkl'.format(name=df)) for df in alldfs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
